{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup enviroment\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleAgent():\n",
    "    def __init__(self, min_lr=0.1, discount=0.95, epsilon=0.1, n_bins = ( 6 , 12 )):\n",
    "        #Initialises with default values if not given.\n",
    "        self.min_lr = min_lr # alpha\n",
    "        self.discount = discount # gamma\n",
    "        self.epsilon = epsilon # exploration probability\n",
    "        self.lower_bounds = []\n",
    "        self.upper_bounds = []\n",
    "        self.n_bins = n_bins\n",
    "    \n",
    "    def initQTable(self, env):\n",
    "        # Convert continous to discrete values\n",
    "        self.qTable = np.zeros(self.n_bins + (env.action_space.n,))\n",
    "        self.lower_bounds = [ env.observation_space.low[2], -math.radians(50) ]\n",
    "        self.upper_bounds = [ env.observation_space.high[2], math.radians(50) ]\n",
    "        \n",
    "    def choose_action(self, state, env, n):\n",
    "        if (np.random.random() < self.exploration_rate(n)):\n",
    "            return env.action_space.sample() \n",
    "        else:\n",
    "            return np.argmax(self.qTable[state])\n",
    "        \n",
    "    def updateQ(self, state, action, reward, new_state, lr):\n",
    "        future_optimal_value = np.max(self.qTable[new_state])\n",
    "        learned_value = reward + self.discount * future_optimal_value\n",
    "        old_value = self.qTable[state][action]\n",
    "        self.qTable[state][action] = (1-lr)*old_value + lr*learned_value \n",
    "        \n",
    "    def learning_rate(self,n):\n",
    "        #Decaying exploration rate\n",
    "        return max(self.min_lr, min(1.0, 1.0 - math.log10((n + 1) / 25)))\n",
    "        \n",
    "    def exploration_rate(self, n):\n",
    "        #Decaying exploration rate\n",
    "        return max(self.epsilon, min(1, 1.0 - math.log10((n  + 1) / 25)))\n",
    "       \n",
    "    def discretize_state(self, cart_position , cart_velocity , pole_angle, pole_velocity ):\n",
    "        est = KBinsDiscretizer(n_bins=self.n_bins, encode='ordinal', strategy='uniform')\n",
    "        est.fit([self.lower_bounds, self.upper_bounds ])\n",
    "        return tuple(map(int,est.transform([[pole_angle, pole_velocity]])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Episode finished after 22 timesteps\n",
      "Episode finished after 17 timesteps\n",
      "Episode finished after 31 timesteps\n",
      "Episode finished after 33 timesteps\n",
      "Episode finished after 35 timesteps\n",
      "Episode finished after 12 timesteps\n",
      "Episode finished after 37 timesteps\n",
      "Episode finished after 17 timesteps\n",
      "Episode finished after 11 timesteps\n",
      "Episode finished after 15 timesteps\n",
      "Episode finished after 22 timesteps\n",
      "Episode finished after 13 timesteps\n",
      "Episode finished after 14 timesteps\n",
      "Episode finished after 16 timesteps\n",
      "Episode finished after 22 timesteps\n",
      "Episode finished after 18 timesteps\n",
      "Episode finished after 45 timesteps\n",
      "Episode finished after 39 timesteps\n",
      "Episode finished after 19 timesteps\n",
      "Episode finished after 32 timesteps\n",
      "Episode finished after 31 timesteps\n",
      "Episode finished after 16 timesteps\n",
      "Episode finished after 27 timesteps\n",
      "Episode finished after 18 timesteps\n",
      "Episode finished after 30 timesteps\n",
      "25\n",
      "Episode finished after 12 timesteps\n"
     ]
    }
   ],
   "source": [
    "agent = CartPoleAgent(min_lr=0.005, discount=1.0, epsilon=0.1)\n",
    "agent.initQTable(env)\n",
    "\n",
    "for i_episode in range(1000):\n",
    "    current_state = agent.discretize_state(*env.reset())\n",
    "    \n",
    "    for t in range(100):\n",
    "        if(i_episode % 25 == 0 and t == 0): print(i_episode)\n",
    "        if(i_episode > 250): env.render()\n",
    "\n",
    "        action = agent.choose_action(current_state, env, i_episode)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        #Convert observation(continous) to discrete values to new_state\n",
    "        new_state = agent.discretize_state(*observation) #*observation unpackages object\n",
    "        lr= agent.learning_rate(i_episode) # decaying learning rate\n",
    "        \n",
    "        #Update QTables with new state values\n",
    "        agent.updateQ(current_state, action, reward, new_state, lr)\n",
    "        \n",
    "        # Set new state as current state\n",
    "        current_state = new_state\n",
    "        \n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "print(\"Finished\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
